{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 9E and 9F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 9E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can write your code here\n",
    "data = np.hstack((X, y.reshape(5000,1)))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 5), (1000, 5), (1000, 5))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_rem,Y_train,Y_rem = train_test_split(X,y,test_size=0.40,random_state=16)\n",
    "X_cv,X_test,Y_cv,Y_test = train_test_split(X_rem,Y_rem,test_size=0.50,random_state=16)\n",
    "\n",
    "X_train.shape,X_cv.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(gamma = 0.001, C = 100)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cv = model.decision_function(X_cv)\n",
    "# f_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rbf(X_i,X_q):\n",
    "#     k = np.exp(-model.gamma * np.linalg.norm(X_i-X_q))\n",
    "    k = np.exp(-model.gamma * np.dot(X_i-X_q, X_i-X_q))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def custom_decision_function(Xcv): #use appropriate parameters\n",
    "    ycv = np.zeros(Xcv.shape[0]).reshape((Xcv.shape[0],1))\n",
    "    for i in tqdm(range(Xcv.shape[0])): \n",
    "        sum = 0\n",
    "        for j in range(model.support_vectors_.shape[0]):\n",
    "            kernal = calculate_rbf(model.support_vectors_[j],Xcv[i])\n",
    "            sum += kernal*model.dual_coef_[0][j]\n",
    "        ycv[i] = sum + model.intercept_\n",
    "    return ycv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 197.24it/s]\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 145.36it/s]\n"
     ]
    }
   ],
   "source": [
    "f_cv_custom = custom_decision_function(X_cv)\n",
    "f_test_custom = custom_decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f_cv_custom - f_cv.reshape(1000,1))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 9F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305, 695, 0.996742671009772, 0.0014347202295552368)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_positive = 0\n",
    "N_negative = 0\n",
    "for i in range(len(Y_cv)):\n",
    "    if Y_cv[i] > 0:\n",
    "        N_positive +=1\n",
    "    else: \n",
    "        N_negative +=1\n",
    "y_positive = (N_positive + 1) / (N_positive + 2)\n",
    "y_negative = 1/(N_negative + 2)\n",
    "\n",
    "N_positive,N_negative,y_positive,y_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.80554548])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_cv_custom[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x,w,b):\n",
    "    a = np.dot(x,w)+b\n",
    "    return 1/(1 + math.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSGDClassifier:\n",
    "    def __init__(self,alpha = 0.0001,N = len(X_train),b = 0,w = np.zeros_like(X_train[0]),eta0 = 0.0001,ephocs = 1):\n",
    "        self.alpha = alpha\n",
    "        self.N = N\n",
    "        self.b = b\n",
    "        self.w = w\n",
    "        self.eta0 = eta0\n",
    "        self.ephocs = ephocs\n",
    "        self.train_loss = []\n",
    "        self.result = []\n",
    "    \n",
    "    def calculateLoss(self,w,b,X,Y):\n",
    "        loss = []\n",
    "        for i in np.arange(0,len(X)):\n",
    "            sig = sigmoid(X[i],w,b)\n",
    "            if Y[i] > 0:\n",
    "                loss.append(-y_positive*np.log(sig) - (1-y_positive)*np.log(1-sig) + self.alpha*np.dot(w,w)/2)\n",
    "            else:\n",
    "                loss.append(-y_negative*np.log(sig) - (1-y_negative)*np.log(1-sig) + self.alpha*np.dot(w,w)/2)\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        initial_loss = self.calculateLoss(w,b,X,Y)\n",
    "#         print('Initial Loss:',initial_loss)\n",
    "        for ep in range(self.ephocs):\n",
    "            for i in range(N):\n",
    "                sig = sigmoid(X[i],self.w,self.b)\n",
    "                w_new = (1- (self.alpha*self.eta0)/self.N)*self.w + self.eta0*X[i]*(Y[i]-sig)\n",
    "                b_new = self.b + self.eta0*(Y[i]-sig)\n",
    "                self.w = w_new\n",
    "                self.b = b_new\n",
    "            next_loss = self.calculateLoss(self.w,self.b,X,Y)\n",
    "            self.train_loss.append(next_loss)\n",
    "            print('-- Epoch: {}, Avg. Train Loss: {}'.format(ep+1,next_loss))\n",
    "            if (next_loss < initial_loss) & ((initial_loss-next_loss)<0.001):\n",
    "                break\n",
    "            initial_loss = next_loss\n",
    "            \n",
    "    def getProbability(self,X):\n",
    "        res = []\n",
    "        for i in range(len(X)):\n",
    "            prob = 1/(1+math.exp(-(self.w*X[i]+self.b)))\n",
    "            res.append(prob)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch: 1, Avg. Train Loss: 0.6079155706248737\n",
      "-- Epoch: 2, Avg. Train Loss: 0.5432836790075528\n",
      "-- Epoch: 3, Avg. Train Loss: 0.49358708840014864\n",
      "-- Epoch: 4, Avg. Train Loss: 0.4547003818858166\n",
      "-- Epoch: 5, Avg. Train Loss: 0.42371468946471313\n",
      "-- Epoch: 6, Avg. Train Loss: 0.39859299715826124\n",
      "-- Epoch: 7, Avg. Train Loss: 0.3778994410869025\n",
      "-- Epoch: 8, Avg. Train Loss: 0.3606085361894483\n",
      "-- Epoch: 9, Avg. Train Loss: 0.3459761556072111\n",
      "-- Epoch: 10, Avg. Train Loss: 0.33345327076465753\n",
      "-- Epoch: 11, Avg. Train Loss: 0.3226281158110312\n",
      "-- Epoch: 12, Avg. Train Loss: 0.3131870226728602\n",
      "-- Epoch: 13, Avg. Train Loss: 0.30488754386278194\n",
      "-- Epoch: 14, Avg. Train Loss: 0.2975397344274488\n",
      "-- Epoch: 15, Avg. Train Loss: 0.29099291865545984\n",
      "-- Epoch: 16, Avg. Train Loss: 0.28512619511616644\n",
      "-- Epoch: 17, Avg. Train Loss: 0.2798415264456194\n",
      "-- Epoch: 18, Avg. Train Loss: 0.27505864189224627\n",
      "-- Epoch: 19, Avg. Train Loss: 0.2707112289054954\n",
      "-- Epoch: 20, Avg. Train Loss: 0.2667440535831429\n",
      "-- Epoch: 21, Avg. Train Loss: 0.2631107589204572\n",
      "-- Epoch: 22, Avg. Train Loss: 0.2597721635902968\n",
      "-- Epoch: 23, Avg. Train Loss: 0.2566949345238134\n",
      "-- Epoch: 24, Avg. Train Loss: 0.2538505416185785\n",
      "-- Epoch: 25, Avg. Train Loss: 0.2512144275145971\n",
      "-- Epoch: 26, Avg. Train Loss: 0.248765342861617\n",
      "-- Epoch: 27, Avg. Train Loss: 0.24648481005728792\n",
      "-- Epoch: 28, Avg. Train Loss: 0.24435668754887577\n",
      "-- Epoch: 29, Avg. Train Loss: 0.2423668134719499\n",
      "-- Epoch: 30, Avg. Train Loss: 0.2405027123436499\n",
      "-- Epoch: 31, Avg. Train Loss: 0.23875335222033492\n",
      "-- Epoch: 32, Avg. Train Loss: 0.23710894251026926\n",
      "-- Epoch: 33, Avg. Train Loss: 0.23556076474355955\n",
      "-- Epoch: 34, Avg. Train Loss: 0.23410103021730055\n",
      "-- Epoch: 35, Avg. Train Loss: 0.23272275967932615\n",
      "-- Epoch: 36, Avg. Train Loss: 0.23141968118067596\n",
      "-- Epoch: 37, Avg. Train Loss: 0.23018614298225082\n",
      "-- Epoch: 38, Avg. Train Loss: 0.2290170389950925\n",
      "-- Epoch: 39, Avg. Train Loss: 0.2279077447035672\n",
      "-- Epoch: 40, Avg. Train Loss: 0.22685406189458995\n",
      "-- Epoch: 41, Avg. Train Loss: 0.22585217081505243\n",
      "-- Epoch: 42, Avg. Train Loss: 0.22489858862016104\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros_like(f_cv_custom[0])\n",
    "b = 0\n",
    "eta0  = 0.0001\n",
    "alpha = 0.0001\n",
    "N = len(f_cv_custom)\n",
    "# f_test_custom = custom_decision_function(X_test)\n",
    "ephocs=60\n",
    "model = CustomSGDClassifier(alpha,N,b,w,eta0,ephocs)\n",
    "%time model.fit(f_cv_custom,Y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting Loss and Epoch\n",
    "plt.plot(model.train_loss, range(len(model.train_loss)))\n",
    "plt.title('Loss VS Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.12246588]), -0.10995353241853036)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w,model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01596275869878887,\n",
       " 0.6256673966097421,\n",
       " 0.8474897736634276,\n",
       " 0.8920350885870294,\n",
       " 0.05398627856798464]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_test = model.getProbability(f_test_custom)\n",
    "prob_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(932, 1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(Y_test)):\n",
    "    if (prob_test[i] > 0.5 and Y_test[i]==1) or (prob_test[i] < 0.5 and Y_test[i]==0):\n",
    "        correct += 1\n",
    "        \n",
    "correct,len(Y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
