{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dnubs-Nnb3cw"
   },
   "source": [
    "# Assignment 6: Apply NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12krXa2pb3c1"
   },
   "source": [
    "<ol>\n",
    "    <li><strong>Apply Multinomial NB on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical, numerical features + preprocessed_eassay (BOW)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical, numerical features + preprocessed_eassay (TFIDF)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>The hyper paramter tuning(find best alpha:smoothing parameter)</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>find the best hyper paramter using k-fold cross validation(use GridsearchCV or RandomsearchCV)/simple cross validation data (write for loop to iterate over hyper parameter values)</li>\n",
    "    <li></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "    <strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/hUv6aEy.jpg' width=300px></li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n",
    "    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "fine the top 20 features from either from feature <font color='red'>Set 1</font> or feature <font color='red'>Set 2</font> using absolute values of `feature_log_prob_ ` parameter of `MultinomialNB` \n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) and print their corresponding feature names\n",
    "    </li>\n",
    "    <li>You need to summarize the results at the end of the notebook, summarize it in the table format\n",
    "        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inzYCIaib3c3"
   },
   "source": [
    "<h1>2. Naive Bayes </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYqCIxzFb3c5"
   },
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dA1jknyqb3c_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['school_state', 'teacher_prefix', 'project_grade_category',\n",
      "       'teacher_number_of_previously_posted_projects', 'project_is_approved',\n",
      "       'clean_categories', 'clean_subcategories', 'essay', 'price'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "\n",
       "  clean_categories                 clean_subcategories  \\\n",
       "0     math_science  appliedsciences health_lifescience   \n",
       "1     specialneeds                        specialneeds   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "data = pandas.read_csv('preprocessed_data.csv')\n",
    "# data.shape\n",
    "print(data.columns)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtVV13Fyb3dH"
   },
   "source": [
    "<h2>1.2 Splitting data into Train and cross validation(or test): Stratified Sampling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-PyprDNb3dI",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((109248, 8), (65548, 8), (21850, 8), (21850, 8))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = data.drop(['project_is_approved'],axis = 1)\n",
    "Y = data[['project_is_approved']]\n",
    "\n",
    "X_train,X_rem,Y_train,Y_rem = train_test_split(X,Y,test_size = 0.40,stratify = Y)\n",
    "X_cv,X_test,Y_cv,Y_test = train_test_split(X_rem,Y_rem,test_size = 0.50,stratify = Y_rem)\n",
    "X.shape,X_train.shape,X_cv.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnZwVNULb3dO"
   },
   "source": [
    "<h2>1.3 Make Data Model Ready: encoding eassay, and project_title</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SczZKiab3dR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65548, 8) (65548, 1)\n",
      "(21850, 8) (21850, 1)\n",
      "(21850, 8) (21850, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_cv.shape, Y_cv.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 10, ngram_range=(1,4), max_features= 5000)\n",
    "vectorizer.fit(X_train['essay'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_essay_bow = vectorizer.transform(X_train['essay'].values)\n",
    "X_test_essay_bow = vectorizer.transform(X_test['essay'].values)\n",
    "X_cv_essay_bow = vectorizer.transform(X_cv['essay'].values)\n",
    "X_essay_features = vectorizer.get_feature_names()\n",
    "\n",
    "print(X_train_essay_bow.shape)\n",
    "print(X_test_essay_bow.shape)\n",
    "print(X_cv_essay_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGy86kgHb3dX"
   },
   "source": [
    "<h2>1.4 Make Data Model Ready: encoding numerical, categorical features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfXkofX1b3da"
   },
   "outputs": [],
   "source": [
    "# encoding categorical features: School State\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['school_state'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_state_ohe = vectorizer.transform(X_train['school_state'].values)\n",
    "X_cv_state_ohe = vectorizer.transform(X_cv['school_state'].values)\n",
    "X_test_state_ohe = vectorizer.transform(X_test['school_state'].values)\n",
    "X_state_features = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_state_ohe.shape, Y_train.shape)\n",
    "print(X_cv_state_ohe.shape, Y_cv.shape)\n",
    "print(X_test_state_ohe.shape, Y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical features: teacher_prefix\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['teacher_prefix'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_teacher_ohe = vectorizer.transform(X_train['teacher_prefix'].values)\n",
    "X_cv_teacher_ohe = vectorizer.transform(X_cv['teacher_prefix'].values)\n",
    "X_test_teacher_ohe = vectorizer.transform(X_test['teacher_prefix'].values)\n",
    "X_teacher_features = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_teacher_ohe.shape, Y_train.shape)\n",
    "print(X_cv_teacher_ohe.shape, Y_cv.shape)\n",
    "print(X_test_teacher_ohe.shape, Y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical features: clean_categories\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['clean_categories'].values) # fit has to happen only on train data\n",
    "# print(vectorizer.get_feature_names())\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_category_ohe = vectorizer.transform(X_train['clean_categories'].values)\n",
    "X_cv_category_ohe = vectorizer.transform(X_cv['clean_categories'].values)\n",
    "X_test_category_ohe = vectorizer.transform(X_test['clean_categories'].values)\n",
    "X_category_features = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_category_ohe.shape, Y_train.shape)\n",
    "print(X_cv_category_ohe.shape, Y_cv.shape)\n",
    "print(X_test_category_ohe.shape, Y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical features: clean_subcategories\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['clean_subcategories'].values) # fit has to happen only on train data\n",
    "# print(vectorizer.get_feature_names())\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_subcategory_ohe = vectorizer.transform(X_train['clean_subcategories'].values)\n",
    "X_cv_subcategory_ohe = vectorizer.transform(X_cv['clean_subcategories'].values)\n",
    "X_test_subcategory_ohe = vectorizer.transform(X_test['clean_subcategories'].values)\n",
    "X_subcategory_features = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_subcategory_ohe.shape, Y_train.shape)\n",
    "print(X_cv_subcategory_ohe.shape, Y_cv.shape)\n",
    "print(X_test_subcategory_ohe.shape, Y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical features: project_grade_category\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(X_train['project_grade_category'].values) #\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['project_grade_category'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_grade_ohe = vectorizer.transform(X_train['project_grade_category'].values)\n",
    "X_cv_grade_ohe = vectorizer.transform(X_cv['project_grade_category'].values)\n",
    "X_test_grade_ohe = vectorizer.transform(X_test['project_grade_category'].values)\n",
    "X_grade_features = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_grade_ohe.shape, Y_train.shape)\n",
    "print(X_cv_grade_ohe.shape, Y_cv.shape)\n",
    "print(X_test_grade_ohe.shape, Y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding numerical features: Price\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "# normalizer.fit(X_train['price'].values)\n",
    "# this will rise an error Expected 2D array, got 1D array instead: \n",
    "# array=[105.22 215.96  96.01 ... 368.98  80.53 709.67].\n",
    "# Reshape your data either using \n",
    "# array.reshape(-1, 1) if your data has a single feature \n",
    "# array.reshape(1, -1)  if it contains a single sample.\n",
    "normalizer.fit(X_train['price'].values.reshape(1,-1))\n",
    "\n",
    "X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(1,-1))\n",
    "X_cv_price_norm = normalizer.transform(X_cv['price'].values.reshape(1,-1))\n",
    "X_test_price_norm = normalizer.transform(X_test['price'].values.reshape(1,-1))\n",
    "X_price_features = ['price']\n",
    "\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_price_norm.shape, Y_train.shape)\n",
    "print(X_cv_price_norm.shape, Y_cv.shape)\n",
    "print(X_test_price_norm.shape, Y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Concatinating all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n",
    "from scipy.sparse import hstack\n",
    "X_tr = hstack((X_train_essay_bow, X_train_state_ohe, X_train_teacher_ohe, X_train_grade_ohe,X_train_category_ohe,X_train_subcategory_ohe, X_train_price_norm)).tocsr()\n",
    "X_cr = hstack((X_cv_essay_bow, X_cv_state_ohe, X_cv_teacher_ohe, X_cv_grade_ohe,X_cv_category_ohe,X_cv_subcategory_ohe, X_cv_price_norm)).tocsr()\n",
    "X_te = hstack((X_test_essay_bow, X_test_state_ohe, X_test_teacher_ohe, X_test_grade_ohe,X_test_category_ohe,X_test_subcategory_ohe, X_test_price_norm)).tocsr()\n",
    "X_feature = X_essay_features + X_state_features + X_teacher_features + X_grade_features + X_category_features + X_subcategory_features + X_price_features\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_tr.shape, Y_train.shape)\n",
    "print(X_cr.shape, Y_cv.shape)\n",
    "print(X_te.shape, Y_test.shape)\n",
    "print('Feature size:',len(X_feature))\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYHPYadYb3dh"
   },
   "source": [
    "<h2>1.5 Appling NB on different kind of featurization as mentioned in the instructions</h2>\n",
    "\n",
    "<br>Apply NB on different kind of featurization as mentioned in the instructions\n",
    "<br> For Every model that you work on make sure you do the step 2 and step 3 of instrucations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Appling NB: BOW featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(cfg,data):\n",
    "    loop = data.shape[0]-data.shape[0]%1000\n",
    "    y_data_predict = []\n",
    "    for i in range(0,loop,1000):\n",
    "        y_data_predict.extend(cfg.predict_proba(data[i:i+1000])[:,1])\n",
    "    if data.shape[0]%1000 !=0:\n",
    "        y_data_predict.extend(cfg.predict_proba(data[loop:])[:,1])\n",
    "        \n",
    "    return y_data_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNwqilFxb3di",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_auc = []\n",
    "cv_auc = []\n",
    "K = [0.5,1,20,40,60,80,90,100]\n",
    "# K = [0.1,0.3,0.5,0.7,0.9,1]\n",
    "for i in tqdm(K):\n",
    "    model = MultinomialNB(alpha = i)\n",
    "    model.fit(X_tr,Y_train)\n",
    "    \n",
    "    Y_train_predict = batch_predict(model,X_tr)\n",
    "    Y_cv_predict = batch_predict(model,X_cr)\n",
    "    \n",
    "    train_auc.append(roc_auc_score(Y_train,Y_train_predict))\n",
    "    cv_auc.append(roc_auc_score(Y_cv,Y_cv_predict))\n",
    "\n",
    "plt.plot(K,train_auc,label = 'Train AUC')\n",
    "plt.plot(K,cv_auc,label= 'CV AUC')\n",
    "\n",
    "plt.scatter(K,train_auc,label = 'Train AUC Points')\n",
    "plt.scatter(K,cv_auc,label = 'CV AUC Points')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha:Hyperparameter')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Error Plots')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "model_bow = MultinomialNB()\n",
    "parameters = {'alpha':[3, 15, 25, 51, 101]}\n",
    "\n",
    "clf_bow = GridSearchCV(model_bow,parameters,cv = 3,n_jobs = -1,scoring = 'roc_auc')\n",
    "clf_bow.fit(X_tr,Y_train)\n",
    "\n",
    "results = pd.DataFrame.from_dict(clf_bow.cv_results_)\n",
    "# print(results.head())\n",
    "results = results.sort_values(['param_alpha'])\n",
    "\n",
    "train_auc= results['mean_train_score']\n",
    "train_auc_std= results['std_train_score']\n",
    "cv_auc = results['mean_test_score'] \n",
    "cv_auc_std= results['std_test_score']\n",
    "alpha =  results['param_alpha']\n",
    "\n",
    "plt.plot(alpha, train_auc, label='Train AUC')\n",
    "# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n",
    "# plt.gca().fill_between(K, train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n",
    "\n",
    "plt.plot(alpha, cv_auc, label='CV AUC')\n",
    "# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n",
    "# plt.gca().fill_between(K, cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n",
    "\n",
    "plt.scatter(alpha, train_auc, label='Train AUC points')\n",
    "plt.scatter(alpha, cv_auc, label='CV AUC points')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Alpha: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Hyper parameter Vs AUC plot\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " best_alpha = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "best_model_bow = MultinomialNB(alpha = best_alpha)\n",
    "best_model_bow.fit(X_tr,Y_train)\n",
    "# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "# not the predicted outputs\n",
    "\n",
    "y_train_pred = batch_predict(best_model_bow, X_tr)    \n",
    "y_test_pred = batch_predict(best_model_bow, X_te)\n",
    "\n",
    "train_fpr, train_tpr, tr_thresholds = roc_curve(Y_train, y_train_pred)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Alpha: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"ERROR PLOTS\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we are writing our own function for predict, with defined thresould\n",
    "# we will pick a threshold that will give the least fpr\n",
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]\n",
    "    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n",
    "    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n",
    "    return t\n",
    "\n",
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(Y_train, predict_with_best_t(y_train_pred, best_t)))\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(Y_test, predict_with_best_t(y_test_pred, best_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Applying Naive Bayes: TfIDF featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Make Data Model Ready: encoding eassay, and project_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_cv.shape, Y_cv.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 10, ngram_range=(1,4), max_features= 5000)\n",
    "vectorizer.fit(X_train['essay'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_essay_tfidf = vectorizer.transform(X_train['essay'].values)\n",
    "X_test_essay_tfidf = vectorizer.transform(X_test['essay'].values)\n",
    "X_cv_essay_tfidf = vectorizer.transform(X_cv['essay'].values)\n",
    "\n",
    "print(X_train_essay_tfidf.shape)\n",
    "print(X_test_essay_tfidf.shape)\n",
    "print(X_cv_essay_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Concatinating all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n",
    "\n",
    "X_tr_tfidf = hstack((X_train_essay_tfidf, X_train_state_ohe, X_train_teacher_ohe, X_train_grade_ohe,X_train_category_ohe,X_train_subcategory_ohe, X_train_price_norm)).tocsr()\n",
    "X_cr_tfidf = hstack((X_cv_essay_tfidf, X_cv_state_ohe, X_cv_teacher_ohe, X_cv_grade_ohe,X_cv_category_ohe,X_cv_subcategory_ohe, X_cv_price_norm)).tocsr()\n",
    "X_te_tfidf = hstack((X_test_essay_tfidf, X_test_state_ohe, X_test_teacher_ohe, X_test_grade_ohe,X_test_category_ohe,X_test_subcategory_ohe, X_test_price_norm)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_tr_tfidf.shape, Y_train.shape)\n",
    "print(X_tr_tfidf.shape, Y_cv.shape)\n",
    "print(X_tr_tfidf.shape, Y_test.shape)\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 Appling NB: TfIdf featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_tfidf = MultinomialNB()\n",
    "parameters = {'alpha':[3, 15, 25, 40, 51, 101]}\n",
    "\n",
    "clf_idf = GridSearchCV(model_tfidf,parameters,cv = 3,n_jobs = -1,scoring = 'roc_auc')\n",
    "clf_idf.fit(X_tr_tfidf,Y_train)\n",
    "\n",
    "results = pd.DataFrame.from_dict(clf_idf.cv_results_)\n",
    "# print(results.head())\n",
    "results = results.sort_values(['param_alpha'])\n",
    "\n",
    "train_auc= results['mean_train_score']\n",
    "train_auc_std= results['std_train_score']\n",
    "cv_auc = results['mean_test_score'] \n",
    "cv_auc_std= results['std_test_score']\n",
    "alpha =  results['param_alpha']\n",
    "\n",
    "plt.plot(alpha, train_auc, label='Train AUC')\n",
    "# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n",
    "# plt.gca().fill_between(K, train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n",
    "\n",
    "plt.plot(alpha, cv_auc, label='CV AUC')\n",
    "# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n",
    "# plt.gca().fill_between(K, cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n",
    "\n",
    "plt.scatter(alpha, train_auc, label='Train AUC points')\n",
    "plt.scatter(alpha, cv_auc, label='CV AUC points')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Alpha: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Hyper parameter Vs AUC plot\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "best_model_tfidf = MultinomialNB(alpha = best_alpha)\n",
    "best_model_tfidf.fit(X_tr_tfidf,Y_train)\n",
    "# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "# not the predicted outputs\n",
    "\n",
    "y_train_pred_tfidf = batch_predict(best_model_tfidf, X_tr_tfidf)    \n",
    "y_test_pred_tfidf = batch_predict(best_model_tfidf, X_te_tfidf)\n",
    "\n",
    "train_fpr_tfidf, train_tpr_tfidf, tr_thresholds_tfidf = roc_curve(Y_train, y_train_pred_tfidf)\n",
    "test_fpr_tfidf, test_tpr_tfidf, te_thresholds_tfidf = roc_curve(Y_test, y_test_pred_tfidf)\n",
    "\n",
    "plt.plot(train_fpr_tfidf, train_tpr_tfidf, label=\"train AUC =\"+str(auc(train_fpr_tfidf, train_tpr_tfidf)))\n",
    "plt.plot(test_fpr_tfidf, test_tpr_tfidf, label=\"test AUC =\"+str(auc(test_fpr_tfidf, test_tpr_tfidf)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Alpha: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"ERROR PLOTS\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lS9hIHdib3dp"
   },
   "source": [
    "<h1>3. Summary</h1>\n",
    "\n",
    "<br> as mentioned in the step 5 of instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Top 20 Features of Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_class_prob_sorted = abs(best_model_bow.feature_log_prob_[0, :]).argsort()[::-1]\n",
    "pos_class_prob_sorted = abs(best_model_bow.feature_log_prob_[1, :]).argsort()\n",
    "\n",
    "print('Printing top 20 Feature for Negative Class:\\n', np.take(X_feature, neg_class_prob_sorted[:20]))\n",
    "print()\n",
    "print('Printing top 20 Feature for Positive Class:\\n',np.take(X_feature, pos_class_prob_sorted[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tabular Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythonmatplotlibtips.blogspot.com/2018/11/matplotlib-only-table.html\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "    \n",
    "col_labels = ['Model','Hyper-Para','AUC']\n",
    "row_labels = ['BOW','TfIDF']\n",
    "table_vals = [['Brute', 50, '0.6690'],['Brute', 40, 0.5727]] \n",
    "# Draw table\n",
    "the_table = plt.table(cellText=table_vals,\n",
    "                      colWidths=[0.1] * 3,\n",
    "                      rowLabels=row_labels,\n",
    "                      colLabels=col_labels,\n",
    "                      loc='center')\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(24)\n",
    "the_table.scale(4, 4)\n",
    "\n",
    "# Removing ticks and spines enables you to get the figure only with table\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', which='both', right=False, left=False, labelleft=False)\n",
    "for pos in ['right','top','bottom','left']:\n",
    "    plt.gca().spines[pos].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6_Assignment_NB_Instructions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
