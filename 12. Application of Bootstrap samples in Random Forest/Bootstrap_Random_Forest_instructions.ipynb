{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjzGopb_YcKR"
   },
   "source": [
    "# Application of Bootstrap samples in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZSCtDI6YcKT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2Y1Z1DoYcKZ"
   },
   "source": [
    " <li> Load the boston house dataset </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBWRNKCDYcKb"
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJ_FwP7xYcKg"
   },
   "source": [
    "### Task: 1\n",
    "<font color='red'><b>Step 1 Creating samples: </b></font> Randomly create 30 samples from the whole boston data points.\n",
    "<ol>\n",
    "<li>Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points</li>\n",
    "<li>Ex: For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly consider we have selected [4, 5, 7, 8, 9, 3] now we will replciate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]</li>\n",
    "<li> we create 30 samples like this </li>\n",
    "<li> Note that as a part of the Bagging when you are taking the random samples make sure each of the sample will have                different set of columns</li>\n",
    "<li> Ex: assume we have 10 columns for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample [7, 9, 1, 4, 5, 6, 2] and so on...</li>\n",
    "<li> Make sure each sample will have atleast 3 feautres/columns/attributes</li>\n",
    "</ol>\n",
    "\n",
    "<font color='red'><b>Step 2 Building High Variance Models on each of the sample and finding train MSE value:</b></font> Build a DecisionTreeRegressor on each of the sample.\n",
    "<ol><li>Build a regression trees on each of 30 samples.</li>\n",
    "<li>computed the predicted values of each data point(506 data points) in your corpus.</li>\n",
    "<li> predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.</li>\n",
    "<li>Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.</li>\n",
    "</ol>\n",
    "\n",
    "<font color='red'><b>Step 3 Calculating the OOB score :</b></font>\n",
    "<ol>\n",
    "<li>Computed the predicted values of each data point(506 data points) in your corpus.</li>\n",
    "<li>Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.</li>\n",
    "<li>Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.</li>\n",
    "</ol>\n",
    "\n",
    "### Task: 2\n",
    "<pre>\n",
    "<font color='red'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "<ol>\n",
    "<li> Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>\n",
    "</pre>\n",
    "### Task: 3\n",
    "<pre>\n",
    "<font color='red'><b>Given a single query point predict the price of house.</b></font>\n",
    "\n",
    "<li>Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] Predict the house price for this point as mentioned in the step 2 of Task 1. </li>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting x values to dataframe\n",
    "data = pd.DataFrame(data=x[:,:], index= range(len(x)), columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bootstrap_random_forest:\n",
    "    def __init__(self,x,y,n = 30):\n",
    "        self.n = n\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.X_n_sample = {}\n",
    "        self.Y_n_sample = {}\n",
    "        self.column_sample_index = {}\n",
    "        self.row_sample_index = {}\n",
    "        self.y_pred = []\n",
    "        self.y_pred_oob = []\n",
    "        \n",
    "    def create_n_samples(self):\n",
    "        data_size_60 = (int)(0.6*self.x.shape[0])\n",
    "        data_size_40 = self.x.shape[0] - data_size_60\n",
    "    \n",
    "        for i in range(self.n):\n",
    "#       column sampling\n",
    "            idx_col = random.sample(range(data.shape[1]),random.randrange(3, data.shape[1]))\n",
    "            self.column_sample_index[i] = idx_col \n",
    "        \n",
    "#       row sampling\n",
    "            idx = random.sample(range(self.x.shape[0]),data_size_60)\n",
    "            idx2 = random.sample(idx,data_size_40)\n",
    "            idx_row = idx + idx2\n",
    "            self.row_sample_index[i] = idx_row\n",
    "        \n",
    "            sample_x = self.x.iloc[idx_row,idx_col].values\n",
    "            sample_y = self.y[idx_row]\n",
    "        \n",
    "            self.X_n_sample[i] = sample_x\n",
    "            self.Y_n_sample[i] = sample_y\n",
    "    \n",
    "    def train_model(self):\n",
    "        y_pred_total = np.zeros(506)\n",
    "        regressor = DecisionTreeRegressor(random_state=0)\n",
    "        for i in range(self.n):\n",
    "            regressor.fit(self.X_n_sample[i],self.Y_n_sample[i])\n",
    "            y_pred_sample = regressor.predict(self.x.iloc[:,self.column_sample_index[i]])\n",
    "        #     print(y_pred_sample)\n",
    "            y_pred_total = np.add(y_pred_sample,y_pred_total)\n",
    "        self.y_pred = (1/30)*y_pred_total\n",
    "    \n",
    "    def train_model_oob(self):\n",
    "        for i in range(self.x.shape[0]):\n",
    "            y_pred_sample = 0\n",
    "            k = 0\n",
    "            regressor = DecisionTreeRegressor(random_state=0)\n",
    "            for j in range(self.n):\n",
    "                if i  not in self.row_sample_index[j]:\n",
    "                    k+=1\n",
    "                    regressor.fit(self.X_n_sample[j],self.Y_n_sample[j])\n",
    "#                     print(self.column_sample_data[j].shape)\n",
    "                    y_pred_sample += regressor.predict(self.x.iloc[:,self.column_sample_index[j]])\n",
    "            #         print(y_pred_sample)\n",
    "            self.y_pred_oob.append((1/k)*y_pred_sample)\n",
    "            \n",
    "    def predict_sample(self,data):\n",
    "        y_pred_total = np.zeros(len(data))\n",
    "        regressor = DecisionTreeRegressor(random_state=0)\n",
    "        for i in range(self.n):\n",
    "            regressor.fit(self.X_n_sample[i],self.Y_n_sample[i])\n",
    "            y_pred_sample = regressor.predict(data[:,self.column_sample_index[i]])\n",
    "        #     print(y_pred_sample)\n",
    "            y_pred_total = np.add(y_pred_sample,y_pred_total)\n",
    "        y_pred = (1/30)*y_pred_total\n",
    "        return y_pred\n",
    "    \n",
    "    def mean_square_error(self,y_orig):\n",
    "        return (1/506)*np.sum(np.subtract(y_orig,self.y_pred) )\n",
    "    \n",
    "    def mean_square_error_oob(self,y_orig):\n",
    "        return (1/506)*np.sum(np.subtract(y_orig,self.y_pred_oob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bootstrap_random_forest(data,y,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Creating samples: Randomly create 30 samples from the whole boston data points.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.create_n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Building High Variance Models on each of the sample and finding train MSE value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08279268706968855"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mean_square_error(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Calculating the OOB score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train_model_oob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-41.21965559590377"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mean_square_error_oob(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [16:02<00:00, 27.60s/it]\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "oob_score = []\n",
    "for i in tqdm(range(35)):\n",
    "    model = Bootstrap_random_forest(data,y,30)\n",
    "    model.create_n_samples()\n",
    "    model.train_model()\n",
    "    mse.append(model.mean_square_error(y))\n",
    "    model.train_model_oob()\n",
    "    oob_score.append(model.mean_square_error_oob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Mean:  0.0005637626761705887 MSE Std:  0.03298793171491998\n",
      "OOB Score Mean:  0.2963369381016723 OOB Score Std:  16.728015839311727\n"
     ]
    }
   ],
   "source": [
    "print('MSE Mean: ',np.array(mse).mean(),'MSE Std: ',np.array(mse).std())\n",
    "print('OOB Score Mean: ',np.array(oob_score).mean(),'OOB Score Std: ',np.array(oob_score).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval of MSE: [ -0.06541210075366938 , 0.06653962610601055 ]\n",
      "Confidence Interval of OOB Score: [ -33.15969474052178 , 33.75236861672513 ]\n"
     ]
    }
   ],
   "source": [
    "print('Confidence Interval of MSE: [',np.array(mse).mean()-2*np.array(mse).std(),',',np.array(mse).mean()+2*np.array(mse).std(),']')\n",
    "print('Confidence Interval of OOB Score: [',np.array(oob_score).mean()-2*np.array(oob_score).std(),',',np.array(oob_score).mean()+2*np.array(oob_score).std(),']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.12333333])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq = np.array([[0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60]])\n",
    "model.predict_sample(xq)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bootstrap_Random_Forest_instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
